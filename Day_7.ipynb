{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLM+2a9VB/ypYAIkjGygkJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rakshitha149/Python_Basics/blob/main/Day_7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jye39sQiBOlo",
        "outputId": "4eb2c2ab-2a93-42c2-fcae-1abf63e590b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ],
      "source": [
        "#Write a Python script that:\n",
        "#1. Use Genism to preprocess data from a sample text file, follow basic procedures like tokenization, stemming, lemmatization etc.\n",
        "!pip install gensim nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "sample_text = \"\"\"\n",
        "Natural language processing (NLP) is a sub-field of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human (natural) languages. NLP tasks include automatic language translation, sentiment analysis, and text summarization.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "tokens = simple_preprocess(sample_text)\n",
        "\n",
        "\n",
        "tokens_without_stopwords = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens_without_stopwords]\n",
        "\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "\n",
        "\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"Tokens after Stopword Removal:\", tokens_without_stopwords)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HO8EiVvBuQj",
        "outputId": "53ec507e-8c38-40d8-f2c7-07ade57fff9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['natural', 'language', 'processing', 'nlp', 'is', 'sub', 'field', 'of', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'nlp', 'tasks', 'include', 'automatic', 'language', 'translation', 'sentiment', 'analysis', 'and', 'text', 'summarization']\n",
            "Tokens after Stopword Removal: ['natural', 'language', 'processing', 'nlp', 'sub', 'field', 'linguistics', 'computer', 'science', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'natural', 'languages', 'nlp', 'tasks', 'include', 'automatic', 'language', 'translation', 'sentiment', 'analysis', 'text', 'summarization']\n",
            "Stemmed Tokens: ['natur', 'languag', 'process', 'nlp', 'sub', 'field', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'nlp', 'task', 'includ', 'automat', 'languag', 'translat', 'sentiment', 'analysi', 'text', 'summar']\n",
            "Lemmatized Tokens: ['natur', 'languag', 'process', 'nlp', 'sub', 'field', 'linguist', 'comput', 'scienc', 'artifici', 'intellig', 'concern', 'interact', 'comput', 'human', 'natur', 'languag', 'nlp', 'task', 'includ', 'automat', 'languag', 'translat', 'sentiment', 'analysi', 'text', 'summar']\n"
          ]
        }
      ]
    }
  ]
}